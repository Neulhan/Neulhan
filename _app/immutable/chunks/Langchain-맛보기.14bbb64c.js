import{s as le,n as Pt}from"./scheduler.b0c1c2c3.js";import{S as pe,i as oe,g as a,s as p,H as V,h as l,x as i,c as o,j as D,C as K,f as e,k as N,a as s}from"./index.b2264e1b.js";function ie($t){let u,Mt=`생성형 AI 는 올해 아주 핫하다.
나는 개발자로 일하면서 꽤 오랜 시간동안 곁눈질 하다가 이제서야 LLM 을 제대로 들여다보고 있다.
LLM 어플리케이션을 만들기 위해 뒤늦게 기술들을 살펴보고 있는 경험을 공유한다.`,j,c,bt="LLM",G,r,Tt=`몇 달 전에 “ChatGPT Prompt Engineering for Developers” 라는 강의를 보면서 LLM 에 대한 개념은 좀 탑재했었다.
LLM 은 Large Language Model 의 약자이다.
LLM 은 쉽게 말해 다음에 나올 단어를 예측하는 거대한 언어모델이다.`,Q,k,Ht=`AI 모델을 직접 훈련시키는 건 비용이 아주 많이 들고, 전문인력이 필요한 일이다.
그런데 LLM 은 prompt engineering 와 fine tuning 을 통해서 간단하게 AI 를 다룰 수 있게 해주었다.`,Y,x,Et="다음에 나올 단어를 예측하는건 LLM 이 하고, 우리는 LLM 에게 맥락만 전달해주면 된다.",Z,m,yt="Langchain",F,v,At=`최근에는 langchain 에 관한 내용을 보았다.
langchain 은 LLM 을 이용해서 어플리케이션을 만들 수 있는 Chain 을 제공한다.`,J,_,wt='<img src="/images/langchain-ecosystem.png" alt="langchain ecosystem"/>',U,C,St="Langchain 은 public/private 한 데이터를 가져와서 사용자의 질문에 대답하거나, 특정 기능을 구현하는데 특화된 프레임워크이다.",W,f,It="Agent",X,L,Rt=`Agent 는 tool 을 직접 사용해서 작업을 수행한다.
예를 들어 “구글에서 검색하기” 라는 함수를 만들어서 툴로 쥐어주면,
자기가 알아서 필요할 때 구글에서 검색해서 결과를 가져다가 쓴다.`,$,g,Bt="verbose 를 True 로 주면 자기가 무슨 생각 하면서 뭘 하고 있는지 줄줄이 읊는데 보고있으면 꽤나 신기하고 재미있다.",tt,h,et,te=`<code class="language-python">tool_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
tools <span class="token operator">=</span> load_tools<span class="token punctuation">(</span>tool_names<span class="token punctuation">)</span>

agent <span class="token operator">=</span> initialize_agent<span class="token punctuation">(</span>
    tools<span class="token punctuation">,</span> llm<span class="token punctuation">,</span> agent<span class="token operator">=</span>AgentType<span class="token punctuation">.</span>ZERO_SHOT_REACT_DESCRIPTION<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span></code>`,nt,d,Ot="Token Limit",st,P,qt=`Token Limit 은 간단하게 말하면 LLM 이 한 번에 처리할 수 있는 용량에 대한 제한이다.<br/>
Prompt 에 모든 정보를 다 넣으면 LLM 이 처리할 수 있는 한도를 넘기기 때문에 처리가 불가능하다.
그래서 전통적인 벡터 유사도에 따른 검색을 같이 활용한다.<br/>
OpenAI 의 <code>gpt-3.5-turbo</code> 의 경우 4K 의 토큰 제한을 가지고 있다.`,at,M,lt,ee=`<code class="language-python"><span class="token punctuation">&#123;</span>
    <span class="token string">"error"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>
        <span class="token string">"message"</span><span class="token punctuation">:</span> <span class="token string">"This model's maximum context length is 4097 tokens. However, your messages resulted in 9129 tokens. Please reduce the length of the messages."</span><span class="token punctuation">,</span>
        <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"invalid_request_error"</span><span class="token punctuation">,</span>
        <span class="token string">"param"</span><span class="token punctuation">:</span> <span class="token string">"messages"</span><span class="token punctuation">,</span>
        <span class="token string">"code"</span><span class="token punctuation">:</span> <span class="token string">"context_length_exceeded"</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code>`,pt,b,zt="Vector Store",ot,T,Vt=`Vector Store 는 벡터 데이터베이스이다.<br/>
전통적인 검색엔진을 구성할 때, 문서들을 벡터화 한 다음에 문서 사이의 벡터 유사도를 통해 유사도가 높은 문서를 추천한다.<br/>
LLM 에 모든 문서를 parameter 로 보낼 수 없기 때문에, Vector Store 에서 한 번 검색한 결과를 LLM 에 넘겨주는 방식을 많이 사용한다.<br/> <code>Pinecone</code> 이라는 툴이 요즘은 대세인 듯 하다.`,it,H,Dt="langchain 에서는 이 Vector Store 를 쓰는 걸 너무너무 쉽고 간단하게 만들어놨다. document 랑 embedding 만 넣으면 그냥 저장이 되고, 꺼내서 쓰는것도 너무 편하다.",ut,E,ct,ne=`<code class="language-python">Pinecone<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>
    documents<span class="token operator">=</span>documents<span class="token punctuation">,</span>
    embedding<span class="token operator">=</span>embeddings<span class="token punctuation">,</span>
    index_name<span class="token operator">=</span><span class="token string">"farmandgood-doc-index"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span></code>`,rt,y,Kt="Embedding",kt,A,Nt=`Vector Store 에 문서를 벡터화 해서 적재하는걸 Embedding 이라고 한다.<br/>
이 과정도 주로 AI 모델을 통해 이루어진다.`,xt,w,jt=`질문에 알맞은 문서를 벡터 검색으로 찾아내기 위해서는 임베딩의 성능도 아주 중요하다. 따라서 LLM 어플리케이션 개발을 할 때는 임베딩 모델을 이것저것 바꿔가면서 테스트해보는 모양이다.<br/>
가장 일반적인 Embedding 모델은 OpenAI 의 ada-02 라고 하는데, 문서를 1536 차원의 벡터로 임베딩해준다.`,mt,S,vt,se=`<code class="language-python">embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span>
    openai_api_key<span class="token operator">=</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span></code>`,_t,I,Gt="Text Splitting",Ct,R,Qt=`1만자 분량의 문서가 있으면 이 문서를 하나의 문서로 Embedding 하지는 않는다.<br/>
문서 안에서도 내용들이 다양하기에, 하나의 문서로 임베딩 되면 유사도 추천이 부정확해질것.<br/>
그래서 문서를 잘라서 Embedding 하도록 하는걸 Text Splitting 이라고 한다.`,ft,B,Yt="1만자 분량의 문서를 1,000 자 단위로, 100 자의 share 영역을 두고 자르는 식이다.",Lt,O,gt,ae=`<code class="language-python">text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>
    chunk_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> separators<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&#92;n&#92;n"</span><span class="token punctuation">,</span> <span class="token string">"&#92;n"</span><span class="token punctuation">,</span> <span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span></code>`,ht,q,Zt="다음 스텝",dt,z,Ft=`이제 Langchain 으로 직접 어플리케이션을 구현하면서 실제로 구현하면서 이슈가 되는 것들을 좀 살펴보자.<br/>
Summarizing, Document QnA 같은 기능은 구현이 너무 쉬워서 이쪽 위주로 어플리케이션을 만들어봐도 좋을 것 같다.`;return{c(){u=a("p"),u.textContent=Mt,j=p(),c=a("h3"),c.textContent=bt,G=p(),r=a("p"),r.textContent=Tt,Q=p(),k=a("p"),k.textContent=Ht,Y=p(),x=a("p"),x.textContent=Et,Z=p(),m=a("h2"),m.textContent=yt,F=p(),v=a("p"),v.textContent=At,J=p(),_=a("p"),_.innerHTML=wt,U=p(),C=a("p"),C.textContent=St,W=p(),f=a("h3"),f.textContent=It,X=p(),L=a("p"),L.textContent=Rt,$=p(),g=a("p"),g.textContent=Bt,tt=p(),h=a("pre"),et=new V(!1),nt=p(),d=a("h3"),d.textContent=Ot,st=p(),P=a("p"),P.innerHTML=qt,at=p(),M=a("pre"),lt=new V(!1),pt=p(),b=a("h3"),b.textContent=zt,ot=p(),T=a("p"),T.innerHTML=Vt,it=p(),H=a("p"),H.textContent=Dt,ut=p(),E=a("pre"),ct=new V(!1),rt=p(),y=a("h3"),y.textContent=Kt,kt=p(),A=a("p"),A.innerHTML=Nt,xt=p(),w=a("p"),w.innerHTML=jt,mt=p(),S=a("pre"),vt=new V(!1),_t=p(),I=a("h3"),I.textContent=Gt,Ct=p(),R=a("p"),R.innerHTML=Qt,ft=p(),B=a("p"),B.textContent=Yt,Lt=p(),O=a("pre"),gt=new V(!1),ht=p(),q=a("h2"),q.textContent=Zt,dt=p(),z=a("p"),z.innerHTML=Ft,this.h()},l(t){u=l(t,"P",{"data-svelte-h":!0}),i(u)!=="svelte-1arc6h6"&&(u.textContent=Mt),j=o(t),c=l(t,"H3",{"data-svelte-h":!0}),i(c)!=="svelte-17pkcfn"&&(c.textContent=bt),G=o(t),r=l(t,"P",{"data-svelte-h":!0}),i(r)!=="svelte-407xd8"&&(r.textContent=Tt),Q=o(t),k=l(t,"P",{"data-svelte-h":!0}),i(k)!=="svelte-1i1fayc"&&(k.textContent=Ht),Y=o(t),x=l(t,"P",{"data-svelte-h":!0}),i(x)!=="svelte-1wrasyr"&&(x.textContent=Et),Z=o(t),m=l(t,"H2",{"data-svelte-h":!0}),i(m)!=="svelte-7pmgah"&&(m.textContent=yt),F=o(t),v=l(t,"P",{"data-svelte-h":!0}),i(v)!=="svelte-ulq2ol"&&(v.textContent=At),J=o(t),_=l(t,"P",{"data-svelte-h":!0}),i(_)!=="svelte-1opsgq8"&&(_.innerHTML=wt),U=o(t),C=l(t,"P",{"data-svelte-h":!0}),i(C)!=="svelte-vdmucu"&&(C.textContent=St),W=o(t),f=l(t,"H3",{"data-svelte-h":!0}),i(f)!=="svelte-11r4c1t"&&(f.textContent=It),X=o(t),L=l(t,"P",{"data-svelte-h":!0}),i(L)!=="svelte-1ovxi98"&&(L.textContent=Rt),$=o(t),g=l(t,"P",{"data-svelte-h":!0}),i(g)!=="svelte-t0n8vg"&&(g.textContent=Bt),tt=o(t),h=l(t,"PRE",{class:!0});var n=D(h);et=K(n,!1),n.forEach(e),nt=o(t),d=l(t,"H3",{"data-svelte-h":!0}),i(d)!=="svelte-ltauy0"&&(d.textContent=Ot),st=o(t),P=l(t,"P",{"data-svelte-h":!0}),i(P)!=="svelte-1fa8tmr"&&(P.innerHTML=qt),at=o(t),M=l(t,"PRE",{class:!0});var Jt=D(M);lt=K(Jt,!1),Jt.forEach(e),pt=o(t),b=l(t,"H3",{"data-svelte-h":!0}),i(b)!=="svelte-14zrsmm"&&(b.textContent=zt),ot=o(t),T=l(t,"P",{"data-svelte-h":!0}),i(T)!=="svelte-ytxmva"&&(T.innerHTML=Vt),it=o(t),H=l(t,"P",{"data-svelte-h":!0}),i(H)!=="svelte-ilktvr"&&(H.textContent=Dt),ut=o(t),E=l(t,"PRE",{class:!0});var Ut=D(E);ct=K(Ut,!1),Ut.forEach(e),rt=o(t),y=l(t,"H3",{"data-svelte-h":!0}),i(y)!=="svelte-1oyhtpt"&&(y.textContent=Kt),kt=o(t),A=l(t,"P",{"data-svelte-h":!0}),i(A)!=="svelte-1acxsnh"&&(A.innerHTML=Nt),xt=o(t),w=l(t,"P",{"data-svelte-h":!0}),i(w)!=="svelte-153zwt2"&&(w.innerHTML=jt),mt=o(t),S=l(t,"PRE",{class:!0});var Wt=D(S);vt=K(Wt,!1),Wt.forEach(e),_t=o(t),I=l(t,"H3",{"data-svelte-h":!0}),i(I)!=="svelte-1dk0b6r"&&(I.textContent=Gt),Ct=o(t),R=l(t,"P",{"data-svelte-h":!0}),i(R)!=="svelte-tf6egm"&&(R.innerHTML=Qt),ft=o(t),B=l(t,"P",{"data-svelte-h":!0}),i(B)!=="svelte-1q1vicy"&&(B.textContent=Yt),Lt=o(t),O=l(t,"PRE",{class:!0});var Xt=D(O);gt=K(Xt,!1),Xt.forEach(e),ht=o(t),q=l(t,"H2",{"data-svelte-h":!0}),i(q)!=="svelte-uq1p97"&&(q.textContent=Zt),dt=o(t),z=l(t,"P",{"data-svelte-h":!0}),i(z)!=="svelte-3n5uzt"&&(z.innerHTML=Ft),this.h()},h(){et.a=null,N(h,"class","language-python"),lt.a=null,N(M,"class","language-python"),ct.a=null,N(E,"class","language-python"),vt.a=null,N(S,"class","language-python"),gt.a=null,N(O,"class","language-python")},m(t,n){s(t,u,n),s(t,j,n),s(t,c,n),s(t,G,n),s(t,r,n),s(t,Q,n),s(t,k,n),s(t,Y,n),s(t,x,n),s(t,Z,n),s(t,m,n),s(t,F,n),s(t,v,n),s(t,J,n),s(t,_,n),s(t,U,n),s(t,C,n),s(t,W,n),s(t,f,n),s(t,X,n),s(t,L,n),s(t,$,n),s(t,g,n),s(t,tt,n),s(t,h,n),et.m(te,h),s(t,nt,n),s(t,d,n),s(t,st,n),s(t,P,n),s(t,at,n),s(t,M,n),lt.m(ee,M),s(t,pt,n),s(t,b,n),s(t,ot,n),s(t,T,n),s(t,it,n),s(t,H,n),s(t,ut,n),s(t,E,n),ct.m(ne,E),s(t,rt,n),s(t,y,n),s(t,kt,n),s(t,A,n),s(t,xt,n),s(t,w,n),s(t,mt,n),s(t,S,n),vt.m(se,S),s(t,_t,n),s(t,I,n),s(t,Ct,n),s(t,R,n),s(t,ft,n),s(t,B,n),s(t,Lt,n),s(t,O,n),gt.m(ae,O),s(t,ht,n),s(t,q,n),s(t,dt,n),s(t,z,n)},p:Pt,i:Pt,o:Pt,d(t){t&&(e(u),e(j),e(c),e(G),e(r),e(Q),e(k),e(Y),e(x),e(Z),e(m),e(F),e(v),e(J),e(_),e(U),e(C),e(W),e(f),e(X),e(L),e($),e(g),e(tt),e(h),e(nt),e(d),e(st),e(P),e(at),e(M),e(pt),e(b),e(ot),e(T),e(it),e(H),e(ut),e(E),e(rt),e(y),e(kt),e(A),e(xt),e(w),e(mt),e(S),e(_t),e(I),e(Ct),e(R),e(ft),e(B),e(Lt),e(O),e(ht),e(q),e(dt),e(z))}}}const re={title:"Langchain 맛보기",description:"귀여운 Langchain 을 새롭게 배우기 시작합니다.",date:"2023-11-08",categories:["langchain","llm"],published:!0};class ke extends pe{constructor(u){super(),oe(this,u,null,ie,le,{})}}export{ke as default,re as metadata};
