import{s as xe,y as Tt,z as ne,n as fe}from"./scheduler.683807b4.js";import{S as Ce,i as ge,r as le,u as pe,v as oe,d as ue,t as ie,w as ce,g as a,s as p,H as Q,h as l,x as i,c as o,j as G,f as e,B as Y,k as Z,a as s}from"./index.949589b5.js";import{M as Le,g as de,a as se}from"./mdsvex.35f0d827.js";import{I as he}from"./img.99105116.js";function be(K){let u,v=`생성형 AI 는 올해 아주 핫하다.
나는 개발자로 일하면서 꽤 오랜 시간동안 곁눈질 하다가 이제서야 LLM 을 제대로 들여다보고 있다.
LLM 어플리케이션을 만들기 위해 뒤늦게 기술들을 살펴보고 있는 경험을 공유한다.`,_,m,c="LLM",r,k,Ht=`몇 달 전에 “ChatGPT Prompt Engineering for Developers” 라는 강의를 보면서 LLM 에 대한 개념은 좀 탑재했었다.
LLM 은 Large Language Model 의 약자이다.
LLM 은 쉽게 말해 다음에 나올 단어를 예측하는 거대한 언어모델이다.`,F,x,wt=`AI 모델을 직접 훈련시키는 건 비용이 아주 많이 들고, 전문인력이 필요한 일이다.
그런데 LLM 은 prompt engineering 와 fine tuning 을 통해서 간단하게 AI 를 다룰 수 있게 해주었다.`,J,f,At="다음에 나올 단어를 예측하는건 LLM 이 하고, 우리는 LLM 에게 맥락만 전달해주면 된다.",U,C,$t="Langchain",W,g,It=`최근에는 langchain 에 관한 내용을 보았다.
langchain 은 LLM 을 이용해서 어플리케이션을 만들 수 있는 Chain 을 제공한다.`,X,N,L,tt,d,St="Langchain 은 public/private 한 데이터를 가져와서 사용자의 질문에 대답하거나, 특정 기능을 구현하는데 특화된 프레임워크이다.",et,h,zt="Agent",nt,b,Rt=`Agent 는 tool 을 직접 사용해서 작업을 수행한다.
예를 들어 “구글에서 검색하기” 라는 함수를 만들어서 툴로 쥐어주면,
자기가 알아서 필요할 때 구글에서 검색해서 결과를 가져다가 쓴다.`,st,M,Bt="verbose 를 True 로 주면 자기가 무슨 생각 하면서 뭘 하고 있는지 줄줄이 읊는데 보고있으면 꽤나 신기하고 재미있다.",at,P,lt,re=`<code class="language-python">tool_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
tools <span class="token operator">=</span> load_tools<span class="token punctuation">(</span>tool_names<span class="token punctuation">)</span>

agent <span class="token operator">=</span> initialize_agent<span class="token punctuation">(</span>
    tools<span class="token punctuation">,</span> llm<span class="token punctuation">,</span> agent<span class="token operator">=</span>AgentType<span class="token punctuation">.</span>ZERO_SHOT_REACT_DESCRIPTION<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span></code>`,pt,y,Ot="Token Limit",ot,E,qt=`Token Limit 은 간단하게 말하면 LLM 이 한 번에 처리할 수 있는 용량에 대한 제한이다.<br/>
Prompt 에 모든 정보를 다 넣으면 LLM 이 처리할 수 있는 한도를 넘기기 때문에 처리가 불가능하다.
그래서 전통적인 벡터 유사도에 따른 검색을 같이 활용한다.<br/>
OpenAI 의 <code>gpt-3.5-turbo</code> 의 경우 4K 의 토큰 제한을 가지고 있다.`,ut,T,it,me=`<code class="language-python"><span class="token punctuation">&#123;</span>
    <span class="token string">"error"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>
        <span class="token string">"message"</span><span class="token punctuation">:</span> <span class="token string">"This model's maximum context length is 4097 tokens. However, your messages resulted in 9129 tokens. Please reduce the length of the messages."</span><span class="token punctuation">,</span>
        <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"invalid_request_error"</span><span class="token punctuation">,</span>
        <span class="token string">"param"</span><span class="token punctuation">:</span> <span class="token string">"messages"</span><span class="token punctuation">,</span>
        <span class="token string">"code"</span><span class="token punctuation">:</span> <span class="token string">"context_length_exceeded"</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span></code>`,ct,H,Vt="Vector Store",rt,w,Dt=`Vector Store 는 벡터 데이터베이스이다.<br/>
전통적인 검색엔진을 구성할 때, 문서들을 벡터화 한 다음에 문서 사이의 벡터 유사도를 통해 유사도가 높은 문서를 추천한다.<br/>
LLM 에 모든 문서를 parameter 로 보낼 수 없기 때문에, Vector Store 에서 한 번 검색한 결과를 LLM 에 넘겨주는 방식을 많이 사용한다.<br/> <code>Pinecone</code> 이라는 툴이 요즘은 대세인 듯 하다.`,mt,A,jt="langchain 에서는 이 Vector Store 를 쓰는 걸 너무너무 쉽고 간단하게 만들어놨다. document 랑 embedding 만 넣으면 그냥 저장이 되고, 꺼내서 쓰는것도 너무 편하다.",_t,$,kt,_e=`<code class="language-python">Pinecone<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>
    documents<span class="token operator">=</span>documents<span class="token punctuation">,</span>
    embedding<span class="token operator">=</span>embeddings<span class="token punctuation">,</span>
    index_name<span class="token operator">=</span><span class="token string">"farmandgood-doc-index"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span></code>`,vt,I,Kt="Embedding",xt,S,Nt=`Vector Store 에 문서를 벡터화 해서 적재하는걸 Embedding 이라고 한다.<br/>
이 과정도 주로 AI 모델을 통해 이루어진다.`,ft,z,Gt=`질문에 알맞은 문서를 벡터 검색으로 찾아내기 위해서는 임베딩의 성능도 아주 중요하다. 따라서 LLM 어플리케이션 개발을 할 때는 임베딩 모델을 이것저것 바꿔가면서 테스트해보는 모양이다.<br/>
가장 일반적인 Embedding 모델은 OpenAI 의 ada-02 라고 하는데, 문서를 1536 차원의 벡터로 임베딩해준다.`,Ct,R,gt,ke=`<code class="language-python">embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span>
    openai_api_key<span class="token operator">=</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span></code>`,Lt,B,Qt="Text Splitting",dt,O,Yt=`1만자 분량의 문서가 있으면 이 문서를 하나의 문서로 Embedding 하지는 않는다.<br/>
문서 안에서도 내용들이 다양하기에, 하나의 문서로 임베딩 되면 유사도 추천이 부정확해질것.<br/>
그래서 문서를 잘라서 Embedding 하도록 하는걸 Text Splitting 이라고 한다.`,ht,q,Zt="1만자 분량의 문서를 1,000 자 단위로, 100 자의 share 영역을 두고 자르는 식이다.",bt,V,Mt,ve=`<code class="language-python">text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>
    chunk_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> separators<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&#92;n&#92;n"</span><span class="token punctuation">,</span> <span class="token string">"&#92;n"</span><span class="token punctuation">,</span> <span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span></code>`,Pt,D,Ft="다음 스텝",yt,j,Jt=`이제 Langchain 으로 직접 어플리케이션을 구현하면서 실제로 구현하면서 이슈가 되는 것들을 좀 살펴보자.<br/>
Summarizing, Document QnA 같은 기능은 구현이 너무 쉬워서 이쪽 위주로 어플리케이션을 만들어봐도 좋을 것 같다.`,Et;return L=new he({props:{src:"https://neulhan-blog.s3.ap-northeast-2.amazonaws.com/images/Langchain-%EB%A7%9B%EB%B3%B4%EA%B8%B0/2023-11-12-18-05-55.png.webp",alt:"langchain ecosystem"}}),{c(){u=a("p"),u.textContent=v,_=p(),m=a("h3"),m.textContent=c,r=p(),k=a("p"),k.textContent=Ht,F=p(),x=a("p"),x.textContent=wt,J=p(),f=a("p"),f.textContent=At,U=p(),C=a("h2"),C.textContent=$t,W=p(),g=a("p"),g.textContent=It,X=p(),N=a("p"),le(L.$$.fragment),tt=p(),d=a("p"),d.textContent=St,et=p(),h=a("h3"),h.textContent=zt,nt=p(),b=a("p"),b.textContent=Rt,st=p(),M=a("p"),M.textContent=Bt,at=p(),P=a("pre"),lt=new Q(!1),pt=p(),y=a("h3"),y.textContent=Ot,ot=p(),E=a("p"),E.innerHTML=qt,ut=p(),T=a("pre"),it=new Q(!1),ct=p(),H=a("h3"),H.textContent=Vt,rt=p(),w=a("p"),w.innerHTML=Dt,mt=p(),A=a("p"),A.textContent=jt,_t=p(),$=a("pre"),kt=new Q(!1),vt=p(),I=a("h3"),I.textContent=Kt,xt=p(),S=a("p"),S.innerHTML=Nt,ft=p(),z=a("p"),z.innerHTML=Gt,Ct=p(),R=a("pre"),gt=new Q(!1),Lt=p(),B=a("h3"),B.textContent=Qt,dt=p(),O=a("p"),O.innerHTML=Yt,ht=p(),q=a("p"),q.textContent=Zt,bt=p(),V=a("pre"),Mt=new Q(!1),Pt=p(),D=a("h2"),D.textContent=Ft,yt=p(),j=a("p"),j.innerHTML=Jt,this.h()},l(t){u=l(t,"P",{"data-svelte-h":!0}),i(u)!=="svelte-1arc6h6"&&(u.textContent=v),_=o(t),m=l(t,"H3",{"data-svelte-h":!0}),i(m)!=="svelte-17pkcfn"&&(m.textContent=c),r=o(t),k=l(t,"P",{"data-svelte-h":!0}),i(k)!=="svelte-407xd8"&&(k.textContent=Ht),F=o(t),x=l(t,"P",{"data-svelte-h":!0}),i(x)!=="svelte-1i1fayc"&&(x.textContent=wt),J=o(t),f=l(t,"P",{"data-svelte-h":!0}),i(f)!=="svelte-1wrasyr"&&(f.textContent=At),U=o(t),C=l(t,"H2",{"data-svelte-h":!0}),i(C)!=="svelte-7pmgah"&&(C.textContent=$t),W=o(t),g=l(t,"P",{"data-svelte-h":!0}),i(g)!=="svelte-ulq2ol"&&(g.textContent=It),X=o(t),N=l(t,"P",{});var n=G(N);pe(L.$$.fragment,n),n.forEach(e),tt=o(t),d=l(t,"P",{"data-svelte-h":!0}),i(d)!=="svelte-vdmucu"&&(d.textContent=St),et=o(t),h=l(t,"H3",{"data-svelte-h":!0}),i(h)!=="svelte-11r4c1t"&&(h.textContent=zt),nt=o(t),b=l(t,"P",{"data-svelte-h":!0}),i(b)!=="svelte-1ovxi98"&&(b.textContent=Rt),st=o(t),M=l(t,"P",{"data-svelte-h":!0}),i(M)!=="svelte-t0n8vg"&&(M.textContent=Bt),at=o(t),P=l(t,"PRE",{class:!0});var Ut=G(P);lt=Y(Ut,!1),Ut.forEach(e),pt=o(t),y=l(t,"H3",{"data-svelte-h":!0}),i(y)!=="svelte-ltauy0"&&(y.textContent=Ot),ot=o(t),E=l(t,"P",{"data-svelte-h":!0}),i(E)!=="svelte-1fa8tmr"&&(E.innerHTML=qt),ut=o(t),T=l(t,"PRE",{class:!0});var Wt=G(T);it=Y(Wt,!1),Wt.forEach(e),ct=o(t),H=l(t,"H3",{"data-svelte-h":!0}),i(H)!=="svelte-14zrsmm"&&(H.textContent=Vt),rt=o(t),w=l(t,"P",{"data-svelte-h":!0}),i(w)!=="svelte-ytxmva"&&(w.innerHTML=Dt),mt=o(t),A=l(t,"P",{"data-svelte-h":!0}),i(A)!=="svelte-ilktvr"&&(A.textContent=jt),_t=o(t),$=l(t,"PRE",{class:!0});var Xt=G($);kt=Y(Xt,!1),Xt.forEach(e),vt=o(t),I=l(t,"H3",{"data-svelte-h":!0}),i(I)!=="svelte-1oyhtpt"&&(I.textContent=Kt),xt=o(t),S=l(t,"P",{"data-svelte-h":!0}),i(S)!=="svelte-1acxsnh"&&(S.innerHTML=Nt),ft=o(t),z=l(t,"P",{"data-svelte-h":!0}),i(z)!=="svelte-153zwt2"&&(z.innerHTML=Gt),Ct=o(t),R=l(t,"PRE",{class:!0});var te=G(R);gt=Y(te,!1),te.forEach(e),Lt=o(t),B=l(t,"H3",{"data-svelte-h":!0}),i(B)!=="svelte-1dk0b6r"&&(B.textContent=Qt),dt=o(t),O=l(t,"P",{"data-svelte-h":!0}),i(O)!=="svelte-tf6egm"&&(O.innerHTML=Yt),ht=o(t),q=l(t,"P",{"data-svelte-h":!0}),i(q)!=="svelte-1q1vicy"&&(q.textContent=Zt),bt=o(t),V=l(t,"PRE",{class:!0});var ee=G(V);Mt=Y(ee,!1),ee.forEach(e),Pt=o(t),D=l(t,"H2",{"data-svelte-h":!0}),i(D)!=="svelte-uq1p97"&&(D.textContent=Ft),yt=o(t),j=l(t,"P",{"data-svelte-h":!0}),i(j)!=="svelte-3n5uzt"&&(j.innerHTML=Jt),this.h()},h(){lt.a=null,Z(P,"class","language-python"),it.a=null,Z(T,"class","language-python"),kt.a=null,Z($,"class","language-python"),gt.a=null,Z(R,"class","language-python"),Mt.a=null,Z(V,"class","language-python")},m(t,n){s(t,u,n),s(t,_,n),s(t,m,n),s(t,r,n),s(t,k,n),s(t,F,n),s(t,x,n),s(t,J,n),s(t,f,n),s(t,U,n),s(t,C,n),s(t,W,n),s(t,g,n),s(t,X,n),s(t,N,n),oe(L,N,null),s(t,tt,n),s(t,d,n),s(t,et,n),s(t,h,n),s(t,nt,n),s(t,b,n),s(t,st,n),s(t,M,n),s(t,at,n),s(t,P,n),lt.m(re,P),s(t,pt,n),s(t,y,n),s(t,ot,n),s(t,E,n),s(t,ut,n),s(t,T,n),it.m(me,T),s(t,ct,n),s(t,H,n),s(t,rt,n),s(t,w,n),s(t,mt,n),s(t,A,n),s(t,_t,n),s(t,$,n),kt.m(_e,$),s(t,vt,n),s(t,I,n),s(t,xt,n),s(t,S,n),s(t,ft,n),s(t,z,n),s(t,Ct,n),s(t,R,n),gt.m(ke,R),s(t,Lt,n),s(t,B,n),s(t,dt,n),s(t,O,n),s(t,ht,n),s(t,q,n),s(t,bt,n),s(t,V,n),Mt.m(ve,V),s(t,Pt,n),s(t,D,n),s(t,yt,n),s(t,j,n),Et=!0},p:fe,i(t){Et||(ue(L.$$.fragment,t),Et=!0)},o(t){ie(L.$$.fragment,t),Et=!1},d(t){t&&(e(u),e(_),e(m),e(r),e(k),e(F),e(x),e(J),e(f),e(U),e(C),e(W),e(g),e(X),e(N),e(tt),e(d),e(et),e(h),e(nt),e(b),e(st),e(M),e(at),e(P),e(pt),e(y),e(ot),e(E),e(ut),e(T),e(ct),e(H),e(rt),e(w),e(mt),e(A),e(_t),e($),e(vt),e(I),e(xt),e(S),e(ft),e(z),e(Ct),e(R),e(Lt),e(B),e(dt),e(O),e(ht),e(q),e(bt),e(V),e(Pt),e(D),e(yt),e(j)),ce(L)}}}function Me(K){let u,v;const _=[K[0],ae];let m={$$slots:{default:[be]},$$scope:{ctx:K}};for(let c=0;c<_.length;c+=1)m=Tt(m,_[c]);return u=new Le({props:m}),{c(){le(u.$$.fragment)},l(c){pe(u.$$.fragment,c)},m(c,r){oe(u,c,r),v=!0},p(c,[r]){const k=r&1?de(_,[r&1&&se(c[0]),r&0&&se(ae)]):{};r&2&&(k.$$scope={dirty:r,ctx:c}),u.$set(k)},i(c){v||(ue(u.$$.fragment,c),v=!0)},o(c){ie(u.$$.fragment,c),v=!1},d(c){ce(u,c)}}}const ae={title:"Langchain 맛보기",description:"생성형 AI 는 올해 아주 핫하다. 나는 개발자로 일하면서 꽤 오랜 시간동안 곁눈질 하다가 이제서야 LLM 을 제대로 들여다보고 있다. LLM 어플리케이션을 만들기 위해 뒤늦게 기술들을 살펴보고 있는 경험을 공유한다.",date:"2023-11-08",categories:["langchain","llm"],published:!0,thumbnail:"https://neulhan-blog.s3.ap-northeast-2.amazonaws.com/images/Langchain-맛보기/2023-11-12-18-05-55.png.webp"};function Pe(K,u,v){return K.$$set=_=>{v(0,u=Tt(Tt({},u),ne(_)))},u=ne(u),[u]}class we extends Ce{constructor(u){super(),ge(this,u,Pe,Me,xe,{})}}export{we as default,ae as metadata};
